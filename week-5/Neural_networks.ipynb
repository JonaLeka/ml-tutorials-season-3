{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "962a26ba",
      "metadata": {
        "id": "962a26ba"
      },
      "source": [
        "# Neural networks\n",
        "\n",
        "This week, we discussed the main principles behind the neurons' communication: if a neuron sends electrochemical signal which is above a certain threshold, the nearby neuron is activated.\n",
        "\n",
        "Fundamentally, **artificial neural networks** (ANN) are quite similar. When we are building ANN, we are using (multiple) building blocks called neurons that can be defined as a mathematical function that takes data as input, performs transformation and produces an output.\n",
        "\n",
        "To better understand the mathematics behind ANN, let's look at a single neuron.\n",
        "\n",
        "## Neuron\n",
        "\n",
        "![neuron](https://miro.medium.com/max/875/1*NZc0TcMCzpgVZXvUdEkqvA.png)\n",
        "\n",
        "The diagram above demonstrates the basic structure of the single neuron (also known as perceptron).\n",
        "When we pass input features through perceptron, each feature ($x_1, x_2, ...$) is multiplied by its weight ($w_1, w_2, ...$). The sum of the multiplication results is then added to the bias ($b$) that can be imagined as a first term independent from the features (*starting value*). The result is then passed through a nonlinear function called **activation** function which produces the output.\n",
        "\n",
        "The whole perceptron training process can be divided into 3 steps:\n",
        "- Forward propogation\n",
        "- Loss calculation\n",
        "- Backward calculation\n",
        "\n",
        "### Foward propogation\n",
        "\n",
        "The forward propogation can be described as a series of computations made to produce a prediction (it is the process we have just described in the previous section).\n",
        "\n",
        "The previously described steps can be expressed mathematically as follows:\n",
        "- The output from the neuron can be written as $z = \\sum_{i = 1}^nw_ix_i + b$\n",
        "- This output is passed through the activation function ($A$) to produce an output, $\\hat{y} = A(z)$\n",
        "\n",
        "Similar to the previous lectures, this produced output is compared to the expected value to calculate loss. But before moving to loss calculation, it might be useful to look at some of the activation functions.\n",
        "\n",
        "##### Activation functions\n",
        "\n",
        "For the simplicity sake, we will not cover all activation functions (at least in this tutorial). Instead, we will focus on two activation functions, we will most likely use in this week's challenge - **reLU** and **sigmoid**.\n",
        "\n",
        "**ReLU** (or rectified linear unit) is a simple function that compares the values with zero. In other words, if the passed value is greater than zero, it will output the value that was passed. Otherwise, the output is zero. In mathematical terms - $A(z) = max(0, z)$.\n",
        "\n",
        "We have already covered **sigmoid function** in the logistic regression tutorial. It can be mathematically expressed in the following way, $A(z) = \\frac{1}{1+exp(-z)}$.\n",
        "\n",
        "Note that the code below is only for educational purpose. Soon, we're going to build a more professional code using a class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02391860",
      "metadata": {
        "id": "02391860"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Defining our activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(z, 0)\n",
        "\n",
        "# forward propagation\n",
        "def forward(X, W, b):\n",
        "    z = X.dot(W) + b\n",
        "    yhat = sigmoid(z)\n",
        "    \n",
        "    return yhat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d46b62",
      "metadata": {
        "id": "04d46b62"
      },
      "source": [
        "### Loss calculation\n",
        "\n",
        "The loss function is a way of mathematicall measuring how good our model prediction is (to later adjust weights and biases).\n",
        "\n",
        "Throughout the series, we are going to introduce a variety of different loss functions, however, for a start let's look to just a few of them.\n",
        "\n",
        "##### Cross-Entropy loss\n",
        "\n",
        "- For the classification tasks, we commonly choose cross-entropy loss.\n",
        "- It can be calculated using the following formula: $loss = -\\sum_{i}^Cy_ilog(\\hat{y_i})$\n",
        "- For the binary classification problem ($C = 2$), such loss function can be written as $loss = -y_1log(\\hat{y_1}) - (1 - y_1)log(1-\\hat{y_1})$\n",
        "\n",
        "\n",
        "##### Mean Squared Error (MSE)\n",
        "- Can be calculated using the following formula: $loss = \\frac{1}{N}\\sum_{i = 1}^n(y_i - \\hat{y_i})^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734a04c2",
      "metadata": {
        "id": "734a04c2"
      },
      "outputs": [],
      "source": [
        "def mse_loss(y, yhat):\n",
        "    num_sample = len(y)\n",
        "\n",
        "    #To avoid assigning the initial value to 0, we are going to use extremely small value\n",
        "    yhat = np.maximum(y_hat, 1e-6)\n",
        "    \n",
        "    loss = - 1/num_sample * (np.subtract(y - yhat)) ^ 2\n",
        "        \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9036f0bf",
      "metadata": {
        "id": "9036f0bf"
      },
      "source": [
        "### Back propogation\n",
        "\n",
        "Back propogation is basically a process of training a neural network by updating its weights and bias. In a nutshell, our model computes predictions that are compared to the expected value which allows to calculate loss function. After some number of epochs, the weights and bias are adjusted in a way that minimizes the loss value, thus ensuring a more accurate predictions.\n",
        "\n",
        "Similar to the previous models, the process of updating coefficients (or in this case, weights and bias) involves calculating loss derivatives in respect to loss functions, multiplying value by the learning rate and subtracting from the previous coefficient value.\n",
        "\n",
        "To better visualize the whole process, let's look at neuron with 2 inputs and sigmoid activation function.\n",
        "\n",
        "![neuron](https://i0.wp.com/neptune.ai/wp-content/uploads/Backpropagation-parameters.png?resize=581%2C361&ssl=1)\n",
        "\n",
        "In such case, the weights and bias would be updated in the following way:\n",
        "- $w_{1new} = w_1 - lr * \\frac{\\partial loss}{\\partial w_1}$\n",
        "\n",
        "- $w_{2new} = w_2 - lr * \\frac{\\partial loss}{\\partial w_2}$\n",
        "\n",
        "- $b_{new} = b - lr * \\frac{\\partial loss}{\\partial b}$\n",
        "\n",
        "On the other hand, coefficients are passed through multiple functions until they reach the final loss value meaning that we will have to use the chain rule.\n",
        "\n",
        "First, let's have a look how it writen for $w_1$. We know that the loss function is initial calculated from the predicted output ($\\hat{y}$), which is calculated by inserting weighted sum ($z$) to sigmoid activation function. Finally, the weighted sum is dependent from the weight in respect to which we are trying to find the derivative. Using the chain rule:\n",
        "- $\\frac{\\partial loss}{\\partial w_1} = \\frac{\\partial loss}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_1}$\n",
        "\n",
        "Similarly, we can find the derivatives for the remaining weights and bias to get the following update equations:\n",
        "\n",
        "- $w_{1new} = w_1 - lr * \\frac{\\partial loss}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_1}$\n",
        "\n",
        "- $w_{2new} = w_2 - lr * \\frac{\\partial loss}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w_2}$\n",
        "\n",
        "- $b_{new} = b - lr * \\frac{\\partial loss}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial b}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9094cf",
      "metadata": {
        "id": "be9094cf"
      },
      "source": [
        "## Multiple layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72ae276",
      "metadata": {
        "id": "d72ae276"
      },
      "source": [
        "So far, we have learned how to build a perceptrons. On the other hand, as you might imagine, does not provide accurate results when applying to large, complex datasets. As the term neural network might suggest, one of the main ways of making our model more sophisticated is using multiple neurons and layers.\n",
        "\n",
        "To better understand how this works, let's look at the example structure of 3 layer neural network.\n",
        "\n",
        "![neural network](https://miro.medium.com/max/875/1*Z3zHoX1nhK6Rsmd4yNPdsg.jpeg)\n",
        "\n",
        "Even though the structure itself looks way more complex, the working principle remains the same. Each neuron has weights for each neuron-neuron connection and each neuron has its bias. The output of each neuron (described in single perceptron section) is passed, weighted and summed at the connected neurons (basically, one neuron's output becomes another's input). After passing all layers, we generate the final output which is then used to measure the loss and start back propagation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together\n",
        "\n",
        "In this exercise, we'll build a simple 2-layer model that classifies iris. Defining each layer like ReLU or Dense in a separate class makes it easy to stack the layers."
      ],
      "metadata": {
        "id": "6xEtZOhPZ5z4"
      },
      "id": "6xEtZOhPZ5z4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d9e288a",
      "metadata": {
        "id": "0d9e288a"
      },
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "    def forward(self, inputs):\n",
        "        return np.maximum(0, inputs)\n",
        "    \n",
        "    def backward(self, inputs, grad_inputs):\n",
        "        positive_indices = inputs > 0\n",
        "        return positive_indices * grad_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxCrossEntropy:\n",
        "    def __init__(self, num_classes=10, is_sparse=False):\n",
        "        self.softmax_outputs = None\n",
        "        self.num_classes = num_classes\n",
        "        self.is_sparse = is_sparse\n",
        "    \n",
        "    def softmax(self, logits):\n",
        "        max_logit = np.max(logits)\n",
        "        exp = np.exp(logits - max_logit) # prevent overfitting\n",
        "        exp_sum = np.sum(exp, axis=-1, keepdims=True)\n",
        "        return exp / exp_sum\n",
        "\n",
        "    def convert_sparse_labels_to_one_hots(self, labels):\n",
        "        \"\"\"\n",
        "        If the \n",
        "        \"\"\"\n",
        "        return np.eye(self.num_classes)[labels]\n",
        "    \n",
        "    def crossentropy_loss(self, predictions, labels):\n",
        "        predictions = np.clip(predictions, 1e-9, 1. - 1e-9)\n",
        "        return - np.sum(labels * np.log(predictions + 1e-9))\n",
        "    \n",
        "    def forward(self, predictions, labels):\n",
        "        \"\"\"\n",
        "        returns a crossentropy loss of softmax\n",
        "        \"\"\"\n",
        "        batch_size = len(predictions)\n",
        "\n",
        "        self.softmax_outputs = self.softmax(predictions)\n",
        "        # print(self.softmax_outputs)\n",
        "\n",
        "        if self.is_sparse:\n",
        "            labels = self.convert_sparse_labels_to_one_hots(labels)\n",
        "\n",
        "        crossentropy_loss = self.crossentropy_loss(predictions, labels) / batch_size\n",
        "        return crossentropy_loss\n",
        "    \n",
        "    def backward(self, inputs, labels):\n",
        "        \"\"\"\n",
        "        returns softmax - labels\n",
        "        \"\"\"\n",
        "        batch_size = len(inputs)\n",
        "\n",
        "        if self.is_sparse:\n",
        "            labels = self.convert_sparse_labels_to_one_hots(labels)\n",
        "\n",
        "        grad = (self.softmax_outputs - labels) / batch_size\n",
        "        return grad"
      ],
      "metadata": {
        "id": "OBg58pxrZi3s"
      },
      "id": "OBg58pxrZi3s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense:\n",
        "    def __init__(self, num_input_units, num_output_units, learning_rate=0.01):\n",
        "        xavier_bound = np.sqrt(6 / (num_input_units + num_output_units))\n",
        "        self.W = np.random.uniform(-xavier_bound, xavier_bound, (num_input_units, num_output_units))\n",
        "        self.b = np.random.rand(num_output_units)\n",
        "        self.learning_rate = learning_rate\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        return inputs @ self.W + self.b\n",
        "    \n",
        "    def backward(self, inputs, grad_inputs):\n",
        "        grad_W = inputs.T @ grad_inputs\n",
        "        grad_b = np.mean(grad_inputs, axis=0)\n",
        "        \n",
        "        self.W -= self.learning_rate * grad_W\n",
        "        self.b -= self.learning_rate * grad_b\n",
        "        \n",
        "        grad_outputs = grad_inputs @ self.W.T\n",
        "        \n",
        "        return grad_outputs"
      ],
      "metadata": {
        "id": "XMPJDnU9Zj8v"
      },
      "id": "XMPJDnU9Zj8v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a00c1151",
      "metadata": {
        "id": "a00c1151"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "        \n",
        "    def predict(self, X):\n",
        "        outputs = X\n",
        "        for layer in self.layers:\n",
        "            outputs = layer.forward(outputs)\n",
        "        \n",
        "        return outputs\n",
        "    \n",
        "    def get_mini_batch(self, X, y, batch_size):\n",
        "        random_indices = np.random.choice(len(X), len(X), replace=False)\n",
        "        shuffled_X = X[random_indices]\n",
        "        shuffled_y = y[random_indices]\n",
        "        \n",
        "        for i in range(0, len(X)-batch_size, batch_size):\n",
        "            yield shuffled_X[i:i+batch_size], shuffled_y[i:i+batch_size]\n",
        "    \n",
        "    def train(self, X, y, num_epochs, batch_size, loss_function):\n",
        "        num_iterations = len(X) // batch_size\n",
        "        \n",
        "        for epoch in range(num_epochs):\n",
        "            batch_generator = self.get_mini_batch(X, y, batch_size)\n",
        "            for iteration in range(num_iterations):\n",
        "                batch_X, batch_y = next(batch_generator)\n",
        "                loss = self.backpropagate(batch_X, batch_y, loss_function)\n",
        "                \n",
        "                if iteration % 3 == 0:\n",
        "                    print(f\"Epoch: {epoch} Iteration: {iteration} / {num_iterations} Loss: {loss}\")\n",
        "\n",
        "            print()\n",
        "            \n",
        "                    \n",
        "    def backpropagate(self, X, y, loss_function):\n",
        "        # get activation for each layer including the input layer\n",
        "        activations = [X]  # initialise with the input layer\n",
        "        for layer in self.layers:\n",
        "            activation = layer.forward(activations[-1])\n",
        "            activations += activation,\n",
        "        \n",
        "        # get loss and its gradient\n",
        "        predictions = activations[-1]\n",
        "        loss = loss_function.forward(predictions, y)\n",
        "        grad = loss_function.backward(predictions, y)\n",
        "        \n",
        "        for layer_index in range(len(self.layers))[::-1]:\n",
        "            layer = self.layers[layer_index]\n",
        "            grad = layer.backward(activations[layer_index], grad)\n",
        "\n",
        "        return np.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train our model on Iris dataset. It's an easy multi-class classification dataset. The task is to classify three different types of iris, given 150 data points with 4 features, "
      ],
      "metadata": {
        "id": "IyRxpANfaSsC"
      },
      "id": "IyRxpANfaSsC"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "dataset = load_iris()\n",
        "\n",
        "x_train = dataset.data\n",
        "y_train = dataset.target\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0H5JGGxaDTj",
        "outputId": "659fcfb4-f0ed-4a34-d861-f04d7ea2091d"
      },
      "id": "u0H5JGGxaDTj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4) (150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = x_train.shape[-1]\n",
        "num_classes = 3"
      ],
      "metadata": {
        "id": "OjowAuI3aHHB"
      },
      "id": "OjowAuI3aHHB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [\n",
        "    Dense(num_features, 32),\n",
        "    ReLU(),\n",
        "    Dense(32, 32),\n",
        "    ReLU(),\n",
        "    Dense(32, num_classes)\n",
        "]"
      ],
      "metadata": {
        "id": "-KK0gL0IaH3V"
      },
      "id": "-KK0gL0IaH3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = NeuralNetwork(layers)\n",
        "network.train(x_train, y_train, num_epochs=4, batch_size=4, loss_function=SoftmaxCrossEntropy(num_classes=num_classes, is_sparse=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_hBj8XCaIyc",
        "outputId": "cde5ea48-856d-4f08-eb71-15d7c211c358"
      },
      "id": "S_hBj8XCaIyc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Iteration: 0 / 37 Loss: 10.729487079160572\n",
            "Epoch: 0 Iteration: 3 / 37 Loss: 15.165377040535766\n",
            "Epoch: 0 Iteration: 6 / 37 Loss: 10.87084199127973\n",
            "Epoch: 0 Iteration: 9 / 37 Loss: 10.726921646096883\n",
            "Epoch: 0 Iteration: 12 / 37 Loss: 10.6314229773482\n",
            "Epoch: 0 Iteration: 15 / 37 Loss: 11.127993131575321\n",
            "Epoch: 0 Iteration: 18 / 37 Loss: 10.3688210302413\n",
            "Epoch: 0 Iteration: 21 / 37 Loss: 6.250562720254374\n",
            "Epoch: 0 Iteration: 24 / 37 Loss: 10.367717317469392\n",
            "Epoch: 0 Iteration: 27 / 37 Loss: 15.200618721084249\n",
            "Epoch: 0 Iteration: 30 / 37 Loss: 5.526456463961456\n",
            "Epoch: 0 Iteration: 33 / 37 Loss: 6.3394019269506625\n",
            "Epoch: 0 Iteration: 36 / 37 Loss: 1.6010766707781028\n",
            "\n",
            "Epoch: 1 Iteration: 0 / 37 Loss: 1.1315527013772961\n",
            "Epoch: 1 Iteration: 3 / 37 Loss: 5.493653808402377\n",
            "Epoch: 1 Iteration: 6 / 37 Loss: 6.4086806027411285\n",
            "Epoch: 1 Iteration: 9 / 37 Loss: 0.6840143648487138\n",
            "Epoch: 1 Iteration: 12 / 37 Loss: 0.34710307245456107\n",
            "Epoch: 1 Iteration: 15 / 37 Loss: 0.1576412415970579\n",
            "Epoch: 1 Iteration: 18 / 37 Loss: 5.32737617405186\n",
            "Epoch: 1 Iteration: 21 / 37 Loss: 0.6711479318899511\n",
            "Epoch: 1 Iteration: 24 / 37 Loss: 15.184002453658511\n",
            "Epoch: 1 Iteration: 27 / 37 Loss: 10.07346228616812\n",
            "Epoch: 1 Iteration: 30 / 37 Loss: 0.46370247160089784\n",
            "Epoch: 1 Iteration: 33 / 37 Loss: 0.3821320182920992\n",
            "Epoch: 1 Iteration: 36 / 37 Loss: 1.036682871759578\n",
            "\n",
            "Epoch: 2 Iteration: 0 / 37 Loss: 5.186933863752559\n",
            "Epoch: 2 Iteration: 3 / 37 Loss: 0.08600319570075557\n",
            "Epoch: 2 Iteration: 6 / 37 Loss: 10.127483653721384\n",
            "Epoch: 2 Iteration: 9 / 37 Loss: 0.4117572478022597\n",
            "Epoch: 2 Iteration: 12 / 37 Loss: 0.8764307166583652\n",
            "Epoch: 2 Iteration: 15 / 37 Loss: 0.41460262438348316\n",
            "Epoch: 2 Iteration: 18 / 37 Loss: 0.8170328032172579\n",
            "Epoch: 2 Iteration: 21 / 37 Loss: 1.268825867448994\n",
            "Epoch: 2 Iteration: 24 / 37 Loss: 0.08175067177883487\n",
            "Epoch: 2 Iteration: 27 / 37 Loss: 0.3151203156922199\n",
            "Epoch: 2 Iteration: 30 / 37 Loss: 0.0\n",
            "Epoch: 2 Iteration: 33 / 37 Loss: 0.28395389284306227\n",
            "Epoch: 2 Iteration: 36 / 37 Loss: 5.007529664096617\n",
            "\n",
            "Epoch: 3 Iteration: 0 / 37 Loss: 1.0003870285952075\n",
            "Epoch: 3 Iteration: 3 / 37 Loss: 0.3028987260905516\n",
            "Epoch: 3 Iteration: 6 / 37 Loss: 0.18408365246222294\n",
            "Epoch: 3 Iteration: 9 / 37 Loss: 0.0\n",
            "Epoch: 3 Iteration: 12 / 37 Loss: 5.007529664096617\n",
            "Epoch: 3 Iteration: 15 / 37 Loss: 1.333377118364561\n",
            "Epoch: 3 Iteration: 18 / 37 Loss: 1.9829695489462944\n",
            "Epoch: 3 Iteration: 21 / 37 Loss: 0.31364741308513194\n",
            "Epoch: 3 Iteration: 24 / 37 Loss: 5.231175757212507\n",
            "Epoch: 3 Iteration: 27 / 37 Loss: 0.17390147222701602\n",
            "Epoch: 3 Iteration: 30 / 37 Loss: 0.05382753637134574\n",
            "Epoch: 3 Iteration: 33 / 37 Loss: 0.1844951212878593\n",
            "Epoch: 3 Iteration: 36 / 37 Loss: 0.9123455447639698\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "440b74a1",
      "metadata": {
        "id": "440b74a1"
      },
      "source": [
        "## Keras\n",
        "\n",
        "There was a lot of work to build this simple model. We had to implement every bit of the model, from cross entropy loss to training loop. Besides, this model is sometimes unstable, often encoutering NaN loss. And what if you want to use fancy techniques like [Batch Normalisation](https://en.wikipedia.org/wiki/Batch_normalization) or [Adam Optimser](https://www.geeksforgeeks.org/intuition-of-adam-optimizer/)? \n",
        "\n",
        "This is where TensorFlow and Keras come to the rescue. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e3dcf05",
      "metadata": {
        "id": "4e3dcf05"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow even lets you load a image dataset called [MNIST](https://www.tensorflow.org/datasets/catalog/mnist). Let's build a simple model of two layers that classifies digits from images. "
      ],
      "metadata": {
        "id": "4wcQVpo_cd_7"
      },
      "id": "4wcQVpo_cd_7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe1b052",
      "metadata": {
        "id": "7fe1b052"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "        \n",
        "# Normalize pixel values\n",
        "x_train = x_train / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e01f7e18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e01f7e18",
        "outputId": "bfb61560-bf4c-4dee-869f-964fe340fb25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 60000 examples with shape (28, 28)\n"
          ]
        }
      ],
      "source": [
        "data_shape = x_train.shape\n",
        "\n",
        "print(f\"There are {data_shape[0]} examples with shape ({data_shape[1]}, {data_shape[2]})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.models.Sequential([ \n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax'),\n",
        "]) "
      ],
      "metadata": {
        "id": "3U0UhZAFJd2R"
      },
      "id": "3U0UhZAFJd2R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy']) "
      ],
      "metadata": {
        "id": "YIbCcoaWJf1U"
      },
      "id": "YIbCcoaWJf1U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model for 10 epochs\n",
        "model.fit(x_train, y_train, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRh5kxzRJiH2",
        "outputId": "dfed0a5a-1cdc-48d9-d49f-f05a1ac49ac9"
      },
      "id": "YRh5kxzRJiH2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8s 3ms/step - loss: 0.2577 - accuracy: 0.9272\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1139 - accuracy: 0.9664\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0789 - accuracy: 0.9761\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0584 - accuracy: 0.9823\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0469 - accuracy: 0.9853\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0359 - accuracy: 0.9887\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0280 - accuracy: 0.9916\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0245 - accuracy: 0.9921\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0192 - accuracy: 0.9938\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0160 - accuracy: 0.9950\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f114039eb10>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With only a few lines of code, we could train a model that classifies digits with 99.5% of accuracy. From now on, we will utilise this wonderful framework to build various models like Recurrent Neural Network, Convolutional Neural Network, etc."
      ],
      "metadata": {
        "id": "3xBc4TUocVp-"
      },
      "id": "3xBc4TUocVp-"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}